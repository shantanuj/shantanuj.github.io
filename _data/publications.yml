# Papers list on home page
papers:
  - title: "TDAM: Top-Down Attention Module for Contextually-Guided Feature Selection in CNNs"
    authors: "Shantanu Jaiswal, Basura Fernando, Cheston Tan"
    venue: "In *European Conference on Computer Vision (ECCV)*, 2022."
    teaser: tdam_teaser.png
    links:
      - "[[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850255.pdf)"
      - "[[Suppl.]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850255-supp.pdf)"
      - "[[Code]](https://github.com/shantanuj/TDAM_Top_down_attention_module)"
    descript: "<ol style='margin-left:-1em;'><li>Introduces a top-down attention formulation to enable models to <i><b>'look again' and attend to salient objects or features</b></i> based on higher-level activations that contain relatively semantically-richer task or contextual information.</li> 
               <li>Improves performance on <i><b>multiple tasks</b></i> (in object recognition and localization) for <i><b>various architecture types</b></i> with few additional parameters and memory consumption </li> (e.g. 5% better top-1 weakly supervised object localization accuracy for ResNet50 on ImageNet).
               <li>Also, makes models <i><b>more robust to changes in input resolution</b></i> and <i><b>reduces entropy of output activations </b></i>(suggesting enhanced feature selectivity; entropy and top-bottom feature co-activation analysis in supplemental).</li></ol>"
   
- title: "Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models"
    authors: "Ishaan Rawal, Shantanu Jaiswal, Basura Fernando, and Cheston Tan"
    venue: "*arXiv preprint*, 2023. Short version in *NeurIPS 2023 XAI in Action Workshop*."
    teaser: quag_teaser.png
    links:
      - "[[Paper]](https://arxiv.org/abs/2306.08889)"
      - "[[Short workshop paper]](https://openreview.net/pdf?id=bhvlGMbONN)"
    descript: "<ol style='margin-left:-1em;'><li>Analyzes multimodal biases in videoQA models through a new <i><b>quadrant-averaging (QUAG) attentional probing method</b></i> and a <i><b>diagnostic benchmark (CLAVI) with temporal counterfactuals.</b></i></li> 
                  <li>Notable finding 1: Model performances drastically drop for counterfactual temporal scenarios suggesting <b><i>lack of temporal understanding</b></i> and poor abilities in current videoQA models to jointly reason on video and language inputs.</li>
                  <li>Notable finding 2: Model performances (on existing benchmarks) are maintained even after impaired (averaged) attention for text and visual modalities, suggesting current models <b><i>may exploit ``shortcuts"</b></i> to achieve competitive performances on existing benchmarks.</li> 
                  </ol>"
  
  - title: "What do CNNs gain by imitating the visual development of primate infants?"
    authors: "Shantanu Jaiswal, Dongkyu Choi, Basura Fernando"
    venue: "In *British Machine Vision Conference (BMVC)*, 2020."
    teaser: grow_cnns_3.png
    links:
      - "[[Paper]](https://www.bmvc2020-conference.com/assets/papers/0196.pdf)"
      - "[[Suppl.]](https://shantanuj.github.io/files/bmvc_supplemental.pdf)"
      - "[[Code]](https://github.com/shantanuj/Imitating-primate-infant-visual-development-CNNs)"
      - "[[Abstract (Cogsci 2020)]](https://www.cognitivesciencesociety.org/cogsci20/papers/0860/0860.pdf)"
    descript: "<ol style='margin-left:-1em;'><li>Analyses a <i><b>growth-based training strategy</b></i> wherein models are grown and inputs are gradually refined over course of training.</li>
                   <li>Growth-based training results in <i><b>higher performance</b></i> (on object recognition) with <i><b>faster training speed</b></i> compared to traditional static training approaches.</li>
                   <li>Growth strategy potentially allows <i><b>'coarse global' patterns</b></i> to be identified first and then <i><b>'finer' ones</b></i> later in a <i><b>more hierarchical</b></i> manner (visualization of induced filters during training in supplemental).</li></ol>"
    
techprojects:
    - title: "A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods"
    authors: "Shantanu Jaiswal, Liu Yan, Dongkyu Choi, Kenneth Kwok"
    teaser: pckr_teaser_2.png
    links:
      - "[[Paper]](https://arxiv.org/abs/2211.16822)"
      - "[Code & data pending agency approval]"
    
  - title: "Domain adaptation techniques for aspect-based sentiment analysis"
    authors: "Shantanu Jaiswal (Final year undergraduate project)"
    teaser: da_fgs.png
    links:
      - "[[Report]](https://dr.ntu.edu.sg/handle/10356/74089)"
      - "[[Code]](https://github.com/shantanuj/Fine_Grained_Sen_Extraction)"
      
  - title: "FakeNet: A Framework to Detect and Analyze Fake News"
    authors: "Shantanu Jaiswal, Abhay Nainan, Jacob Sunny, Sharique Zaman"
    teaser: stance_project.png
    links:
      - "[[Report]](https://shantanuj.github.io/files/FakeNet_report.pdf)"
      - "[[Code]](https://github.com/shantanuj/Fake_Net)"