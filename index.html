<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shantanu Jaiswal</title>

    <meta name="author" content="Shantanu Jaiswal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shantanu Jaiswal
                </p>
                <p>
		I'm a MSR student at CMU <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, advised by <a href="https://www.cs.cmu.edu/~dpathak/">Prof. Deepak Pathak</a>. Previously, I was a researcher at the <a href="https://www.a-star.edu.sg/cfar">Center for Frontier AI Research</a> at A*STAR Singapore, where I was advised by <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Dr. Cheston Tan</a> and <a href="https://basurafernando.github.io/">Dr. Basura Fernando</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sjaiswa3@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Shantanu_Jaiswal.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=GmGNq2MAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/shantanuj">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Shantanu_Jaiswal.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Shantanu_Jaiswal.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm broadly interested in deep learning, computer vision and cognitive science. My recent research focuses on architectural refinements and learning methods towards more capable multimodal generative models.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                  <tbody>
                    <tr>
                      <td style="padding:0px;width:20%;vertical-align:middle">
                        <img src='images/iter-refine-img.png' width="190" alt="Iterative refinement figure">
                      </td>
                      <td style="padding:0px 0px 0px 40px;width:80%;vertical-align:middle">
                        <a href="files/comp_iter_refine_preprint.pdf">
                          <span class="papertitle">Iterative Refinement Improves Compositional Image Generation</span>
                        </a>
                        <br>
                        <strong>Shantanu Jaiswal</strong>,
                        <a href="https://mihirp1998.github.io/">Mihir Prabhudesai</a>,
                        <a href="https://nikashbhardwaj.com/">Nikash Bhardwaj</a>,
                        <a href="https://openreview.net/profile?id=%7EZheyang_Qin1">Zheyang Qin</a>,
                        <a href="https://scholar.google.com/citations?user=MQFngiMAAAAJ&hl=en">Amir Zadeh</a>,
                        <a href="https://github.com/chuanli11">Chuan Li</a>,
                        <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>,
                        <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a>
                        <br>
                        <em>Under review at CVPR</em>, 2026
                        <br>
                        <a href="files/comp_iter_refine_preprint.pdf">Preprint</a>
                        /
                        code (soon)
                        <p></p>
                        <p>
                          A training-free iterative refinement mechanism to improve compositional image generation capabilities of text-to-image models (incl. NanoBanana, GPT-Image-1 and Qwen-Image).
                        </p>
                      </td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                  <tbody>
                    <tr>
                      <td style="padding:0px;width:20%;vertical-align:middle">
                        <img src='images/iprm5.png' width="190" alt="IPRM figure">
                      </td>
                      <td style="padding:0px 0px 0px 40px;width:80%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2411.13754">
                          <span class="papertitle">Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios</span>
                        </a>
                        <br>
                        <strong>Shantanu Jaiswal</strong>,
                        <a href="https://sites.google.com/view/debadityaroy/home">Debaditya Roy</a>,
                        <a href="https://basurafernando.github.io/">Basura Fernando</a>,
                        <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Cheston Tan</a>
                        <br>
                        <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2411.13754">arXiv</a>
                        /
                        <a href="https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism">code</a>
                        <p></p>
                        <p>
                          A hybrid recurrent-transformer module to improve compositional visual reasoning capabilities of vision-language backbones. Achieves state-of-art on multiple image and video question answering benchmarks (incl. STAR, AGQA, CLEVR-/CLEVRER-Humans).
                        </p>
                      </td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>

            
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                  <tbody>
                    <tr>
                      <td style="padding:0px;width:20%;vertical-align:middle">
                        <img src='images/quag.png' width="190" alt="QUAG figure">
                      </td>
                      <td style="padding:0px 0px 0px 40px;width:80%;vertical-align:middle">
                        <a href="https://dissect-videoqa.github.io/">
                          <span class="papertitle">Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion</span>
                        </a>
                        <br>
                        <a href="https://israwal.github.io/">Ishaan Rawal</a>,
                        <a href="https://scholar.google.com/citations?user=9GDRHp8AAAAJ&hl=en">Alexander Matyasko</a>,
                        <strong>Shantanu Jaiswal</strong>,
                        <a href="https://basurafernando.github.io/">Basura Fernando</a>,
                        <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Cheston Tan</a>
                        <br>
                        <em>International Conference on Machine Learning (ICML)</em>, 2024
                        <br>

                        <a href="https://arxiv.org/abs/2306.08889">arXiv</a>
                        /
                        <a href="https://github.com/israwal/dissect-videoqa">code</a>
                        <p></p>
                        <p>
                          An analysis of multimodal biases in VideoQA transformer models through a non-parametric probe and a stress-test dataset suggesting strong modality-specific biases in existing models.
                        </p>
                      </td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>
            
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                  <tbody>
                    <tr>
                      <td style="padding:0px;width:20%;vertical-align:middle">
                        <img src='images/tdam.png' width="190" alt="TDAM figure">
                      </td>
                      <td style="padding:0px 0px 0px 40px;width:80%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2111.13470">
                          <span class="papertitle">TDAM: Top-Down Attention Module for Contextually-Guided Feature Selection in CNNs</span>
                        </a>
                        <br>
                        <strong>Shantanu Jaiswal</strong>,
                        <a href="https://basurafernando.github.io/">Basura Fernando</a>,
                        <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Cheston Tan</a>
                        <br>
                        <em>European Conference on Computer Vision (ECCV)</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2111.13470">arXiv</a>
                        /
                        <a href="https://github.com/shantanuj/TDAM_Top-Down_Attention_Module">code</a>
                        <p></p>
                        <p>
                          A lightweight top-down attention module that iteratively uses higher-level features to attend to lower-level features across model hierarchy. Improves fine-grained object recognition and localization performances of CNN vision backbones.
                        </p>
                      </td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>

            

            


          </tbody></table>

          
					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                  <br>
                  Last updated: November 2025
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
